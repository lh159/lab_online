import os
import re
from typing import List, Dict, Any
from funasr import AutoModel
from funasr.utils.postprocess_utils import rich_transcription_postprocess


class ASRService:
    """
    å°è£… ASR æ¨ç†é€»è¾‘ï¼Œæä¾› transcribe(path) è¿”å›è¯†åˆ«æ–‡æœ¬åŠæŒ‰å¥/è¯çš„ç½®ä¿¡åº¦ä¿¡æ¯ã€‚
    """

    def __init__(self,
                 model_origin_path: str = "/root/demo_1_confidence/xu_zhuxi_model/SenseVoiceSmall",
                 model_trained_path: str = "/root/demo_1_confidence/xu_zhuxi_model/SenseVoiceSmall",
                 device: str = "cuda:0"):
        self.model_origin_path = model_origin_path
        self.model_trained_path = model_trained_path
        self.device = device
        self.modelOR = None
        self.modelTR = None
        self._load_models()

    def _load_models(self):
        print("ASRService: loading models...")
        # åŠ è½½åŸå§‹æ¨¡å‹
        self.modelOR = AutoModel(
            model=self.model_origin_path,
            trust_remote_code=False,
            vad_model=None,
            vad_kwargs={"max_single_segment_time": 30000},
            device=self.device,
        )
        # åŠ è½½è®­ç»ƒåæ¨¡å‹ï¼ˆå¦‚æœä¸ origin ç›¸åŒä¹Ÿå¯ä»¥ï¼‰
        self.modelTR = AutoModel(
            model=self.model_trained_path,
            trust_remote_code=False,
            vad_model=None,
            vad_kwargs={"max_single_segment_time": 30000},
            device=self.device,
        )

    def _postprocess_text(self, text: str) -> str:
        text = rich_transcription_postprocess(text)
        # æ¸…ç†å¸¸è§ emoji
        for e in ["ğŸ˜Š", "ğŸ¼", "ğŸ˜”"]:
            text = text.replace(e, "")
        return text

    def _split_sentences(self, text: str) -> List[str]:
        # ç®€å•æŒ‰ä¸­æ–‡å’Œè‹±æ–‡å¥æœ«ç¬¦å·åˆ†å¥ï¼Œä¿ç•™æ ‡ç‚¹
        parts = re.split(r'([ã€‚ï¼ï¼Ÿ!?]+)', text)
        sentences = []
        for i in range(0, len(parts) - 1, 2):
            sent = (parts[i] + parts[i + 1]).strip()
            if sent:
                sentences.append(sent)
        # å¤„ç†æœ«å°¾æ®‹ä½™
        if len(parts) % 2 == 1 and parts[-1].strip():
            sentences.append(parts[-1].strip())
        return sentences if sentences else [text]

    def _avg_conf(self, probs: List[float]) -> float:
        """
        æ›´å¥å£®åœ°è®¡ç®—å¹³å‡ç½®ä¿¡åº¦ã€‚æ”¯æŒå…ƒç´ ä¸ºæ•°å€¼æˆ–å­—å…¸çš„æƒ…å†µã€‚
        å­—å…¸ä¼šå°è¯•æŒ‰å¸¸è§å­—æ®µæå–æ•°å€¼ï¼š'prob','confidence','score','p','probability'ã€‚
        æ— æ³•è§£æçš„å…ƒç´ ä¼šè¢«è·³è¿‡ï¼›è‹¥æ‰€æœ‰å…ƒç´ æ— æ³•è§£æåˆ™è¿”å› 0.0ã€‚
        """
        if not probs:
            return 0.0
        return float(sum(probs) / len(probs))

    def transcribe(self, audio_path: str, use_trained: bool = False) -> Dict[str, Any]:
        """
        å¯¹éŸ³é¢‘æ–‡ä»¶æ‰§è¡Œæ¨ç†ï¼Œè¿”å›ç»“æ„ï¼š
        {
          "text": "...",
          "sentences": [{"text": "...", "confidence": 0.95, "words": [{"text": "...","confidence":0.9}, ...]}, ...],
          "raw_prob": [...],  # åŸå§‹æ¨¡å‹è¿”å›çš„ç½®ä¿¡åº¦åˆ—è¡¨ï¼ˆå¯èƒ½ä¸º []ï¼‰
        }
        """
        model = self.modelTR if use_trained else self.modelOR
        res = model.generate(
            input=audio_path,
            cache={},
            language="auto",
            use_itn=True,
            batch_size_s=60,
            merge_vad=True,
            merge_length_s=15,
        )

        raw_text = res[0].get("text", "")
        raw_prob = res[0].get("prob", []) or []
        text = self._postprocess_text(raw_text)

        # å­—ç¬¦çº§æ¦‚ç‡é•¿åº¦å¯èƒ½ä¸ text ä¸ä¸€è‡´ï¼Œå®‰å…¨å¤„ç†
        char_probs = []
        # å…ˆå°† raw_prob ä¸­å¯èƒ½çš„ dict å…ƒç´ è½¬æ¢ä¸ºæ•°å€¼åˆ—è¡¨
        numeric_raw_prob: List[float] = []
        if raw_prob:
            # å¦‚æœ raw_prob çš„å…ƒç´ ä¸º dict æˆ–å¤æ‚ç»“æ„ï¼Œå°è¯•æå–æ•°å€¼
            converted = []
            for el in raw_prob:
                if isinstance(el, (int, float)):
                    converted.append(float(el))
                elif isinstance(el, dict):
                    # å°è¯•æå–å¸¸è§å­—æ®µ
                    v = None
                    for key in ("prob", "confidence", "score", "p", "probability"):
                        if key in el:
                            v = el[key]
                            break
                    if isinstance(v, (int, float)):
                        converted.append(float(v))
                    elif isinstance(v, list):
                        nums = [float(x) for x in v if isinstance(x, (int, float))]
                        converted.append(sum(nums) / len(nums) if nums else 0.0)
                    elif isinstance(v, dict):
                        found = None
                        for subkey in ("prob", "confidence", "score", "p", "probability"):
                            if subkey in v and isinstance(v[subkey], (int, float)):
                                found = float(v[subkey])
                                break
                        converted.append(found if found is not None else 0.0)
                    else:
                        converted.append(0.0)
                else:
                    try:
                        converted.append(float(el))
                    except Exception:
                        converted.append(0.0)

            # å¦‚æœ converted ä¸­å…¨éƒ¨ä¸º 0.0ï¼ˆæˆ–é•¿åº¦ä¸º0ï¼‰ï¼Œä¿ç•™åŸ raw_prob empty
            numeric_raw_prob = converted

        if len(numeric_raw_prob) == len(raw_text):
            # å‡è®¾ raw_prob ä¸ raw_text å¯¹åº”ï¼ˆæœ€å¸¸è§ï¼‰
            # æŠŠ raw_prob å¯¹é½åˆ°å¤„ç†å textï¼ˆç®€å•ç­–ç•¥ï¼šæˆªæ–­æˆ–ç”¨æœ€åå€¼å¡«å……ï¼‰
            if len(text) == len(raw_text):
                char_probs = numeric_raw_prob
            else:
                # å¤„ç†é•¿åº¦ä¸ç­‰æ—¶ï¼Œå°è¯•æŒ‰æ¯”ä¾‹æ˜ å°„
                if numeric_raw_prob:
                    scale = len(numeric_raw_prob) / max(1, len(text))
                    for i in range(len(text)):
                        idx = min(int(i * scale), len(numeric_raw_prob) - 1)
                        char_probs.append(numeric_raw_prob[idx])
        else:
            # fallback: å¦‚æœæ²¡æœ‰ prob æˆ–é•¿åº¦ä¸åŒ¹é…ï¼Œä½¿ç”¨å‡å€¼ 0.9ï¼ˆä¿å®ˆï¼‰
            if numeric_raw_prob:
                avg = self._avg_conf(numeric_raw_prob)
                char_probs = [avg] * len(text)
            elif raw_prob:
                # raw_prob å­˜åœ¨ä½†æ— æ³•è§£æä¸ºæ•°å€¼ï¼Œå°è¯•ä»åŸå§‹ç»“æ„å–å¹³å‡
                avg = self._avg_conf(raw_prob)
                char_probs = [avg] * len(text)
            else:
                char_probs = [0.9] * len(text)

        # åˆ†å¥å¹¶è®¡ç®—å¥å­ä¸è¯ç½®ä¿¡åº¦ï¼ˆæŒ‰å­—ç¬¦å¹³å‡ï¼‰
        sentences = []
        sent_boundaries = []
        # è®¡ç®—æ¯ä¸ªå¥å­çš„å­—ç¬¦èŒƒå›´ï¼ˆç®€å•æŒ‰åˆ†å¥å‡½æ•°ï¼‰
        sents = self._split_sentences(text)
        cursor = 0
        for s in sents:
            length = len(s)
            sent_probs = char_probs[cursor:cursor + length] if cursor + length <= len(char_probs) else char_probs[cursor:]
            sent_conf = self._avg_conf(sent_probs)
            # è¯çº§ï¼ˆç”¨ç©ºç™½åˆ†è¯ï¼‰
            words = []
            word_cursor = 0
            for w in s.split():
                wlen = len(w)
                w_probs = sent_probs[word_cursor:word_cursor + wlen] if word_cursor + wlen <= len(sent_probs) else sent_probs[word_cursor:]
                words.append({"text": w, "confidence": round(self._avg_conf(w_probs), 4)})
                word_cursor += wlen + 1  # +1 for the space removed by split (approx)
            sentences.append({"text": s, "confidence": round(sent_conf, 4), "words": words})
            cursor += length

        return {"text": text, "sentences": sentences, "raw_prob": raw_prob}


